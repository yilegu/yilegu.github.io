<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Yi (Yile) Gu – 顾亦乐</title>
  <style>
    /* Base page styling similar to ysymyth.github.io */
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #ffffff;
      color: #333;
      line-height: 1.6;
    }
    .wrapper {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
    }
    header {
      text-align: center;
      padding: 40px 20px;
    }
    header h1 {
      margin: 0;
      font-size: 2.5em;
    }
    header .chinese-name {
      margin: 5px 0;
      font-size: 1.6em;
      color: #666;
    }
    header p.role {
      margin: 5px 0 10px;
      font-size: 1.2em;
      color: #555;
    }
    nav {
      text-align: center;
      margin-top: 15px;
    }
    nav a {
      margin: 0 8px;
      color: #0077aa;
      text-decoration: none;
    }
    nav a:hover {
      text-decoration: underline;
    }
    section {
      margin-top: 40px;
    }
    section h2 {
      border-bottom: 1px solid #ddd;
      padding-bottom: 10px;
      margin-bottom: 20px;
      font-size: 1.6em;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
    li {
      margin-bottom: 12px;
    }
    footer {
      margin-top: 40px;
      text-align: center;
      padding: 20px;
      border-top: 1px solid #eee;
      color: #666;
    }
  </style>
</head>
<body>
  <div class="wrapper">
    <header>
      <h1>Yi (Yile) Gu</h1>
      <div class="chinese-name">顾亦乐</div>
      <p class="role">Principal Applied Scientist, Amazon</p>
      <nav>
        <!-- Provide links to professional profiles similar to the navigation icons on ysymyth.github.io -->
        <a href="https://scholar.google.com/citations?user=mnxh0UMAAAAJ&hl=en&oi=ao">Google Scholar</a> |
        <a href="https://github.com/yilegu">GitHub</a> |
        <a href="https://www.amazon.science/author/yi-gu">Amazon Science</a>
      </nav>
    </header>

    <section id="about">
      <h2>About</h2>
      <p>I am a principal applied scientist at Amazon focusing on cutting‑edge methods for conversational AI, speech recognition and machine learning. According to my <a href="https://www.amazon.science/author/yi-gu">Amazon Science profile</a>, I have contributed to fifteen publications and two blog posts. My research spans Conversational AI, machine learning and security/privacy, with tags such as automatic speech recognition (ASR), speech, natural‑language processing (NLP), natural‑language understanding (NLU) and neural networks【668896757278115†L141-L166】. I work on scalable language models for speech recognition, leveraging multi‑modal data and advanced optimization techniques to improve both efficiency and accuracy.</p>
    </section>

    <section id="selected-work">
      <h2>Selected work</h2>
      <ul>
        <li><strong>Speech recognition rescoring with large speech‑text foundation models (ICASSP 2025)</strong> – Demonstrates how multi‑modal large language models combining speech and text can enhance automatic speech recognition by providing a second‑pass rescoring and leveraging large amounts of data【668896757278115†L226-L229】.</li>
        <li><strong>Towards ASR robust spoken language understanding through in‑context learning with word confusion networks (ICASSP 2024)</strong> – Applies natural language understanding techniques to spoken language tasks by feeding transcribed speech into large language models and addressing challenges due to noisy ASR output【668896757278115†L244-L247】.</li>
        <li><strong>Paralinguistics‑enhanced large language modeling of spoken dialogue (ICASSP 2024)</strong> – Investigates ways to incorporate paralinguistic cues such as sentiment, emotion and speaking style into large language models to achieve more human‑like spoken dialogue【668896757278115†L261-L264】.</li>
        <li><strong>Align‑SLM: Textless spoken language models with reinforcement learning from AI feedback</strong> – A reinforcement learning framework that improves textless spoken language models by optimizing them with AI‑provided feedback, achieving state‑of‑the‑art performance on audio generation and non‑speech tasks【292725430128537†L37-L64】.</li>
        <li><strong>Multi‑modal retrieval for large language model‑based speech recognition</strong> – Introduces multi‑modal retrieval combining k‑nearest‑neighbor language models and cross‑attention to dramatically improve automatic speech recognition and achieve state‑of‑the‑art performance【674814154262576†L50-L60】.</li>
        <li><strong>Scaling laws for discriminative speech recognition rescoring models</strong> – Shows that the word error rate of speech recognition models follows a scaling law in relation to training data and model size and demonstrates that pre‑trained models can achieve better performance with less training data【954391350198922†L49-L62】.</li>
        <li><strong>Personalization for BERT‑based discriminative speech recognition rescoring</strong> – Explores personalization through gazetteers, prompting and cross‑attention, achieving over 10% improvement in word error rate, with prompts improving performance without additional training【209670084688595†L50-L61】.</li>
        <li><strong>Mitigating closed‑model adversarial examples with Bayesian neural modeling for enhanced end‑to‑end speech recognition</strong> – Proposes a Bayesian neural network‑based detector to identify and mitigate adversarial examples, improving detection rates and reducing word error rate in speech recognition【660011539618013†L51-L64】.</li>
      </ul>
    </section>

    <section id="talks">
      <h2>Online talks & resources</h2>
      <p>For recent talks and presentations, please refer to my Amazon Science author page and Google Scholar profile.</p>
    </section>

    <footer>
      <p>&copy; 2025 Yi (Yile) Gu – 顾亦乐. All rights reserved.</p>
    </footer>
  </div>
</body>
</html>