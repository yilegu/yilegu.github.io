<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Yi (Yile) Gu – 顾亦乐</title>
  <style>
    /* Base page styling similar to ysymyth.github.io */
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #ffffff;
      color: #333;
      line-height: 1.6;
    }
    .wrapper {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
    }
    header {
      text-align: center;
      padding: 40px 20px;
    }
    header h1 {
      margin: 0;
      font-size: 2.5em;
    }
    header .chinese-name {
      margin: 5px 0;
      font-size: 1.6em;
      color: #666;
    }
    header p.role {
      margin: 5px 0 10px;
      font-size: 1.2em;
      color: #555;
    }
    nav {
      text-align: center;
      margin-top: 15px;
    }
    nav a {
      margin: 0 8px;
      color: #0077aa;
      text-decoration: none;
    }
    nav a:hover {
      text-decoration: underline;
    }
    section {
      margin-top: 40px;
    }
    section h2 {
      border-bottom: 1px solid #ddd;
      padding-bottom: 10px;
      margin-bottom: 20px;
      font-size: 1.6em;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
    li {
      margin-bottom: 12px;
    }
    footer {
      margin-top: 40px;
      text-align: center;
      padding: 20px;
      border-top: 1px solid #eee;
      color: #666;
    }
  </style>
</head>
<body>
  <div class="wrapper">
    <header>
      <h1>Yi (Yile) Gu</h1>
      <div class="chinese-name">顾亦乐</div>
      <p class="role">Principal Applied Scientist, Amazon</p>
      <nav>
        <!-- Provide links to professional profiles similar to the navigation icons on ysymyth.github.io -->
        <a href="https://scholar.google.com/citations?user=mnxh0UMAAAAJ&hl=en&oi=ao">Google Scholar</a> |
        <a href="https://github.com/yilegu">GitHub</a> |
        <a href="https://www.amazon.science/author/yi-gu">Amazon Science</a>
      </nav>
    </header>

    <section id="about">
      <h2>About</h2>
      <p>I am a principal applied scientist at Amazon focusing on cutting‑edge methods for conversational AI, speech recognition and machine learning. According to my <a href="https://www.amazon.science/author/yi-gu">Amazon Science profile</a>, I have contributed to fifteen publications and two blog posts. My research spans Conversational AI, machine learning and security/privacy, with tags such as automatic speech recognition (ASR), speech, natural‑language processing (NLP), natural‑language understanding (NLU) and neural networks【668896757278115†L141-L166】. I work on scalable language models for speech recognition, leveraging multi‑modal data and advanced optimization techniques to improve both efficiency and accuracy.</p>
    </section>

    <section id="selected-work">
      <h2>Selected work</h2>
      <ul>
<<ul>
        <li><strong>RescoreBERT: Discriminative speech recognition rescoring with BERT</strong> – Introduces a BERT-based second‑pass ASR rescoring model trained with minimum‑WER and matching word error distribution losses. The model fuses masked language model pre‑training with discriminative training and reduces word error rate by 6.6%/3.4% on LibriSpeech clean/other and cuts latency and WER by 3–8% on an internal dataset【365925939638881†L19-L32】.</li>
        <li><strong>Align‑SLM: Textless spoken language models with reinforcement learning from AI feedback</strong> – Improves textless spoken language models by generating multiple speech continuations, using semantic metrics to create preference data and applying Direct Preference Optimization. The preference optimization enhances semantic understanding and achieves state‑of‑the‑art results on ZeroSpeech 2021 and StoryCloze benchmarks【839558094362318†L13-L34】.</li>
       <li><strong>Scaling laws for discriminative speech recognition rescoring models</strong> – Demonstrates that the word error rate of RescoreBERT rescoring models follows a power‑law relationship with training data size and model size. Pre‑trained models require less data than randomly initialized models due to effective data transfer, which itself follows a scaling law【782709216811046†L16-L23】【782709216811046†L64-L78】.</li>
        <li><strong>Computationally generated constitutive models for particle phase rheology in gas‑fluidized suspensions</strong> – Develops a solid rheology for gas‑fluidized suspensions and integrates it into kinetic theory to predict the formation of particle agglomerates and clusters. The model captures four fluidization regimes (bubbling, bubbling–clustering, bubble‑less and stagnant) by balancing shear and yield stresses【6242550965869†L149-L166】.</li>
      </ul>
    <section id="talks">
      <h2>Online talks & resources</h2>
      <p>For recent talks and presentations, please refer to my Amazon Science author page and Google Scholar profile.</p>
    </section>

    <footer>
      <p>&copy; 2025 Yi (Yile) Gu – 顾亦乐. All rights reserved.</p>
    </footer>
  </div>
</body>
</html>
